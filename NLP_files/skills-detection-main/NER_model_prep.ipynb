{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted Data/ner.json to Data/ner.pkl.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "def json_to_pkl(json_file_path, pkl_file_path):\n",
    "    \"\"\"\n",
    "    Converts a JSON file to PKL format.\n",
    "\n",
    "    Args:\n",
    "        json_file_path (str): Path to the input JSON file.\n",
    "        pkl_file_path (str): Path to save the output PKL file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open and read the JSON file\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        \n",
    "        # Save the data to a PKL file\n",
    "        with open(pkl_file_path, 'wb') as pkl_file:\n",
    "            pickle.dump(data, pkl_file)\n",
    "        \n",
    "        print(f\"Successfully converted {json_file_path} to {pkl_file_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "path = \"Data/ner.json\"\n",
    "json_to_pkl(path, \"Data/ner.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': \"Data Engineer (all genders) Babbel 10117 Berlin Babbel is driven by a mission: Everyone. Learning. Languages. This means building products that help people connect and communicate across cultures. Babbel, Babbel Travel and Babbel for Business focus on using a new language in the real world, in real situations, with real people. And it works: Studies with Yale University, City University of New York and Michigan State University prove that it gets users talking with confidence.  The key is a blend of humanity and technology. A team of linguistic experts hand-craft each of our courses, to provide content that is constantly adapted to our learners’ needs. Interactive content with videos and podcasts makes understanding a new language easy, because Babbel is for everyone. That’s why our team is as diverse as our content. From headquarters in Berlin and New York, 750 people from more than 60 nationalities represent the backgrounds, characteristics and perspectives that make all humans unique. Making a true connection with millions of paid subscribers across the globe is what makes Babbel the most profitable language learning app worldwide.  Babbel in Numbers: The #1 top-grossing language learning app globally Millions of active paying subscribers More than 60,000 lessons 14 languages hand-crafted by 150+ linguists More than 10,000 hours of learning content Over 70 learning podcast episodes and over 50 hours of video learning content We are looking for a Data Engineer to join our team in Berlin!  You will join the Data Product Engineering team and work in close collaboration with Babbel's Machine Learning and Product Analytics teams. As a Data Engineer you enable them by designing and building data pipelines and data models, building scalable backends based on the latest AWS services, deploying and operating data products and machine learning models, and exploring new technologies. Your work deepens our understanding of our learners as well as our level of automation, for example in A/B testing. Your architecture and your code accelerate our journey into machine learning for recommendation, personalisation and prediction, impacting millions of language learners across the globe. You are eager to share your experience with younger engineers, and you help take the teams to the next level in DevOps and engineering best practices.  You have  Computer science or related engineering degree and 3+ years professional experience as data engineer Very strong skills and industry experience in modern Python (data classes, generators, type annotations, etc.) and Python Data Stack Experience building and maintaining batch and real-time streaming data pipelines, with SQL and NoSQL data sources and destinations Experience with AWS services (Kinesis, DynamoDB, Lambda functions, S3, etc.), operations and architecture, especially with respect to data heavy applications Strong DevOps experience and commitment to engineering best practices, with an eye for pragmatism and the specific requirements of data and machine learning teams Nice to have  Experience with Snowflake Experience with Apache Airflow Experience with AWS SageMaker Interest in Machine Learning, and experience building ML-driven or algorithmic data products Exposure to product analytics data pipelines and basic understanding of A/B testing Why Babbel?  A steep learning-curve – personally and professionally A challenging environment and a lot of responsibility to prove your skills A lively startup atmosphere with friendly working hours A vibrant international company with people from over 60 different nationalities Great company benefits (30 vacation days, unlimited access to Babbel account, high-end company bike, fresh fruit and drinks, English/German classes, In-house Training Academy) Diversity at Babbel  Babbel is committed to providing a work culture of belonging where every Babbelonian can contribute, participate and learn. Striving to become a truly diverse, inclusive, and equitable company is reflected in our internal policies, such as flexible working hours, the options to work from home, as well as a nap, family, and faith rooms in our headquarter office. At the same time, we acknowledge that creating a culture of belonging is a continuous journey that includes every current and future Babbelonian.  Sounds good? We are looking forward to hearing from you! \",\n",
       " 'annotation': [{'start': 0,\n",
       "   'end': 13,\n",
       "   'label': 'SKILLS',\n",
       "   'text': 'Data Engineer'},\n",
       "  {'start': 1717, 'end': 1731, 'label': 'SKILLS', 'text': 'data pipelines'},\n",
       "  {'start': 1736, 'end': 1747, 'label': 'SKILLS', 'text': 'data models'},\n",
       "  {'start': 1796, 'end': 1808, 'label': 'SKILLS', 'text': 'AWS services'},\n",
       "  {'start': 1852, 'end': 1868, 'label': 'SKILLS', 'text': 'machine learning'},\n",
       "  {'start': 2012, 'end': 2023, 'label': 'SKILLS', 'text': 'A/B testing'},\n",
       "  {'start': 2052, 'end': 2056, 'label': 'SKILLS', 'text': 'code'},\n",
       "  {'start': 2085, 'end': 2101, 'label': 'SKILLS', 'text': 'machine learning'},\n",
       "  {'start': 2251, 'end': 2255, 'label': 'SKILLS', 'text': 'with'},\n",
       "  {'start': 2324, 'end': 2330, 'label': 'SKILLS', 'text': 'DevOps'},\n",
       "  {'start': 2335, 'end': 2346, 'label': 'SKILLS', 'text': 'engineering'},\n",
       "  {'start': 2374,\n",
       "   'end': 2390,\n",
       "   'label': 'EDUCATION',\n",
       "   'text': 'Computer science'},\n",
       "  {'start': 2402, 'end': 2413, 'label': 'SKILLS', 'text': 'engineering'},\n",
       "  {'start': 2425, 'end': 2433, 'label': 'EXPERIENCE', 'text': '3+ years'},\n",
       "  {'start': 2461, 'end': 2474, 'label': 'SKILLS', 'text': 'data engineer'},\n",
       "  {'start': 2528, 'end': 2534, 'label': 'SKILLS', 'text': 'Python'},\n",
       "  {'start': 2590, 'end': 2596, 'label': 'SKILLS', 'text': 'Python'},\n",
       "  {'start': 2674, 'end': 2688, 'label': 'SKILLS', 'text': 'data pipelines'},\n",
       "  {'start': 2690, 'end': 2694, 'label': 'SKILLS', 'text': 'with'},\n",
       "  {'start': 2695, 'end': 2698, 'label': 'SKILLS', 'text': 'SQL'},\n",
       "  {'start': 2703, 'end': 2708, 'label': 'SKILLS', 'text': 'NoSQL'},\n",
       "  {'start': 2750, 'end': 2754, 'label': 'SKILLS', 'text': 'with'},\n",
       "  {'start': 2755, 'end': 2767, 'label': 'SKILLS', 'text': 'AWS services'},\n",
       "  {'start': 2769, 'end': 2776, 'label': 'SKILLS', 'text': 'Kinesis'},\n",
       "  {'start': 2778, 'end': 2786, 'label': 'SKILLS', 'text': 'DynamoDB'},\n",
       "  {'start': 2788, 'end': 2804, 'label': 'SKILLS', 'text': 'Lambda functions'},\n",
       "  {'start': 2806, 'end': 2808, 'label': 'SKILLS', 'text': 'S3'},\n",
       "  {'start': 2857, 'end': 2861, 'label': 'SKILLS', 'text': 'with'},\n",
       "  {'start': 2904, 'end': 2910, 'label': 'SKILLS', 'text': 'DevOps'},\n",
       "  {'start': 2940, 'end': 2951, 'label': 'SKILLS', 'text': 'engineering'},\n",
       "  {'start': 2984, 'end': 2994, 'label': 'SOFT-SKILLS', 'text': 'pragmatism'},\n",
       "  {'start': 3037, 'end': 3053, 'label': 'SKILLS', 'text': 'machine learning'},\n",
       "  {'start': 3085, 'end': 3089, 'label': 'SKILLS', 'text': 'with'},\n",
       "  {'start': 3090, 'end': 3099, 'label': 'SKILLS', 'text': 'Snowflake'},\n",
       "  {'start': 3111, 'end': 3115, 'label': 'SKILLS', 'text': 'with'},\n",
       "  {'start': 3116, 'end': 3130, 'label': 'SKILLS', 'text': 'Apache Airflow'},\n",
       "  {'start': 3147, 'end': 3160, 'label': 'SKILLS', 'text': 'AWS SageMaker'},\n",
       "  {'start': 3173, 'end': 3189, 'label': 'SKILLS', 'text': 'Machine Learning'},\n",
       "  {'start': 3215, 'end': 3217, 'label': 'SKILLS', 'text': 'ML'},\n",
       "  {'start': 3266, 'end': 3283, 'label': 'SKILLS', 'text': 'product analytics'},\n",
       "  {'start': 3284, 'end': 3298, 'label': 'SKILLS', 'text': 'data pipelines'},\n",
       "  {'start': 3326, 'end': 3337, 'label': 'SKILLS', 'text': 'A/B testing'},\n",
       "  {'start': 3743, 'end': 3750, 'label': 'LANGUAGE', 'text': 'English'},\n",
       "  {'start': 3751, 'end': 3757, 'label': 'LANGUAGE', 'text': 'German'}],\n",
       " 'user_input': ''}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pickle.load(open('Data/ner.pkl', 'rb'))\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entities': [(0, 13, 'SKILLS'), (1717, 1731, 'SKILLS'), (1736, 1747, 'SKILLS'), (1796, 1808, 'SKILLS'), (1852, 1868, 'SKILLS'), (2012, 2023, 'SKILLS'), (2052, 2056, 'SKILLS'), (2085, 2101, 'SKILLS'), (2251, 2255, 'SKILLS'), (2324, 2330, 'SKILLS'), (2335, 2346, 'SKILLS'), (2374, 2390, 'EDUCATION'), (2402, 2413, 'SKILLS'), (2425, 2433, 'EXPERIENCE'), (2461, 2474, 'SKILLS'), (2528, 2534, 'SKILLS'), (2590, 2596, 'SKILLS'), (2674, 2688, 'SKILLS'), (2690, 2694, 'SKILLS'), (2695, 2698, 'SKILLS'), (2703, 2708, 'SKILLS'), (2750, 2754, 'SKILLS'), (2755, 2767, 'SKILLS'), (2769, 2776, 'SKILLS'), (2778, 2786, 'SKILLS'), (2788, 2804, 'SKILLS'), (2806, 2808, 'SKILLS'), (2857, 2861, 'SKILLS'), (2904, 2910, 'SKILLS'), (2940, 2951, 'SKILLS'), (2984, 2994, 'SOFT-SKILLS'), (3037, 3053, 'SKILLS'), (3085, 3089, 'SKILLS'), (3090, 3099, 'SKILLS'), (3111, 3115, 'SKILLS'), (3116, 3130, 'SKILLS'), (3147, 3160, 'SKILLS'), (3173, 3189, 'SKILLS'), (3215, 3217, 'SKILLS'), (3266, 3283, 'SKILLS'), (3284, 3298, 'SKILLS'), (3326, 3337, 'SKILLS'), (3743, 3750, 'LANGUAGE'), (3751, 3757, 'LANGUAGE')]}\n"
     ]
    }
   ],
   "source": [
    "train_data2 = []\n",
    "for k in range(len(train_data)):\n",
    "    new_elements = (train_data[k]['document'], { 'entities': [tuple(d.values())[:3] for d in train_data[k]['annotation']]})\n",
    "    #print(new_elements)\n",
    "    train_data2.append(new_elements) \n",
    "print(train_data2[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "Losses: {'ner': 19845.127718293952}\n",
      "Epoch 2/15\n",
      "Losses: {'ner': 11518.848632735397}\n",
      "Epoch 3/15\n",
      "Losses: {'ner': 9246.412289732489}\n",
      "Epoch 4/15\n",
      "Losses: {'ner': 8107.543223173709}\n",
      "Epoch 5/15\n",
      "Losses: {'ner': 7418.729802750902}\n",
      "Epoch 6/15\n",
      "Losses: {'ner': 6890.216871345054}\n",
      "Epoch 7/15\n",
      "Losses: {'ner': 6384.49674207217}\n",
      "Epoch 8/15\n",
      "Losses: {'ner': 6101.940094810865}\n",
      "Epoch 9/15\n",
      "Losses: {'ner': 5861.615441802797}\n",
      "Epoch 10/15\n",
      "Losses: {'ner': 5533.575753756328}\n",
      "Epoch 11/15\n",
      "Losses: {'ner': 5310.5133907144855}\n",
      "Epoch 12/15\n",
      "Losses: {'ner': 4959.1984590189795}\n",
      "Epoch 13/15\n",
      "Losses: {'ner': 4791.42874350153}\n",
      "Epoch 14/15\n",
      "Losses: {'ner': 4596.223288790489}\n",
      "Epoch 15/15\n",
      "Losses: {'ner': 4398.479946490141}\n"
     ]
    }
   ],
   "source": [
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "for _, annotations in train_data2:\n",
    "    for ent in annotations[\"entities\"]:\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "examples = []\n",
    "for text, annotations in train_data2:\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, annotations)\n",
    "    examples.append(example)\n",
    "\n",
    "# Train with batch strategy\n",
    "batch_sizes = compounding(4.0, 32.0, 1.001)\n",
    "# Disable other components in the pipeline during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):  # Only train NER\n",
    "    optimizer = nlp.create_optimizer()\n",
    "    \"\"\"\n",
    "    optimizer.learn_rate = 0.001\n",
    "    for epoch in range(15):  # Set the number of epochs\n",
    "        print(f\"Epoch {epoch + 1}/{15}\")\n",
    "        losses = {}  # Initialize a dictionary to store losses\n",
    "        for example in examples:\n",
    "            nlp.update([example], sgd=optimizer, losses=losses)\n",
    "        print(f\"Losses: {losses}\") \"\"\"\n",
    "    for epoch in range(15):\n",
    "        random.shuffle(train_data2)\n",
    "        print(f\"Epoch {epoch + 1}/15\")\n",
    "        losses = {}\n",
    "        batches = minibatch(examples, size=batch_sizes)\n",
    "        for batch in batches:\n",
    "            nlp.update(batch, sgd=optimizer, losses=losses)\n",
    "            #evaluate_model(nlp, texts, true_labe\n",
    "        print(f\"Losses: {losses}\")\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
